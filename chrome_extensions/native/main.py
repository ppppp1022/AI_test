import sys
import json
import logging
from plyer import notification

import requests
from bs4 import BeautifulSoup

import google.generativeai as genai

_available_webpage = ['www.mk.co.kr', 'www.joongang.co.kr', 'www.hani.co.kr', 'www.donga.com']

# ğŸ”§ ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    filename='native_host.log',
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

def send_notification(message):
    try:
        notification.notify(title='ì•Œë¦¼', message=message, timeout=3)
        logging.info(f'Notification sent: {message}')
    except Exception as e:
        logging.error(f'Notification error: {e}')

def read_message():
    try:
        raw_length = sys.stdin.buffer.read(4)
        if not raw_length:
            logging.warning('No raw length received.')
            return None
        message_length = int.from_bytes(raw_length, byteorder='little')
        message = sys.stdin.buffer.read(message_length).decode('utf-8')
        logging.info(f'Message received: {message}')
        return json.loads(message)
    except Exception as e:
        logging.error(f'Error reading message: {e}')
        return None


def crawl_news_article(url):
    '''
    ì£¼ì–´ì§„ URLì˜ ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜.
    ë§¤ì¼ê²½ì œ, ì¤‘ì•™ì¼ë³´, í•œê²¨ë ˆ, ë™ì•„ì¼ë³´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

    Args:
        url (str): ê¸°ì‚¬ URL

    Returns:
        str: ì¶”ì¶œëœ ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸, ì‹¤íŒ¨ ë˜ëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹ ë¬¸ì‚¬ì¼ ê²½ìš° ë©”ì‹œì§€ ë°˜í™˜
    '''
    try:
        # ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        logging.info(f'Start crawling from {url}')
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}) # User-Agent ì¶”ê°€í•˜ì—¬ ë´‡ ì°¨ë‹¨ ë°©ì§€ ì‹œë„
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ

        # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')

        # --- ì‹ ë¬¸ì‚¬ë³„ ë³¸ë¬¸ ì„ íƒì ì •ì˜ ---
        # ì œê³µë°›ì€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¬¸ì‚¬ë³„ ë³¸ë¬¸ ì˜ì—­ì˜ CSS ì„ íƒìë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        article_selectors = {
            'mk.co.kr': {'selector': 'div.sec_body', 'name': 'ë§¤ì¼ê²½ì œ'},
            'joongang.co.kr': {'selector': 'div.article_body', 'name': 'ì¤‘ì•™ì¼ë³´'}, # id='article_body'ë„ ê°€ëŠ¥í•˜ì§€ë§Œ classê°€ ë” ì¼ë°˜ì 
            'hani.co.kr': {'selector': 'div.article-text', 'name': 'í•œê²¨ë ˆ'},
            'donga.com': {'selector': 'section.news_view', 'name': 'ë™ì•„ì¼ë³´'},
            # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì— ë‹¤ë¥¸ ì‹ ë¬¸ì‚¬ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        }

        # URLì„ í†µí•´ ì‹ ë¬¸ì‚¬ íŒë‹¨ ë° ì„ íƒì ì°¾ê¸°
        target_selector = None
        news_site_name = 'ì•Œ ìˆ˜ ì—†ìŒ'
        for site_keyword, selector_info in article_selectors.items():
            if site_keyword in url:
                target_selector = selector_info['selector']
                news_site_name = selector_info['name']
                break

        if not target_selector:
            logging.warning(f'{url} is not available url.')
            return ''

        logging.info(f'"{news_site_name}" attempt to extract article body (selector: {target_selector})')

        # ì„ íƒìë¡œ ë³¸ë¬¸ ìš”ì†Œ ì°¾ê¸°
        article_element = soup.select_one(target_selector)

        if article_element: 
            # ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•˜ìœ„ íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë„ í¬í•¨)
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ(ì˜ˆ: ê´‘ê³ , ê¸°ì ì •ë³´ ëë¶€ë¶„ ë“±)ëŠ” ì¶”ê°€ì ì¸ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            article_body = article_element.get_text(separator='\n', strip=True)

            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì˜ˆ: ë¹ˆ ì¤„ ì œê±°)
            article_body = '\n'.join([line.strip() for line in article_body.splitlines() if line.strip()])

            return article_body
        else:
            logging.warning(f'{news_site_name} - Cannot fine')
            return f'{news_site_name} - Could not find article body element. (selector: {target_selector})'

    except requests.exceptions.RequestException as e:
        logging.error(f'An error occurred while retrieving URL: {e}')
        return ''
    except Exception as e:
        logging.error(f'An error occurred while crawling: {e}')
        return ''

# ë©”ì¸ ë£¨í”„
logging.info('Native host script started.')

while True:
    msg = read_message()
    if msg is None:
        logging.info('No message or terminated. Exiting loop.')
        break
    url = msg.get('url', '')
    logging.info(f'{msg}')
    logging.info(f'URL checked: {url}')
    splited_url = url.split('/')
    if splited_url[2] in _available_webpage:
        if splited_url[-1].isdigit():
            logging.info(f'Crawling available for {url}')
            crawled = crawl_news_article(url)
            with open('crawled text.txt', 'w') as f:
                f.write(crawled)
        else:
            logging.info(f'{url} is not article.')


logging.info('Native host script exited.')
